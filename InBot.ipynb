{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfkUDPvE5G7G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers,models\n",
        "import json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/drive/MyDrive/Induction_Bot'"
      ],
      "metadata": {
        "id": "-iTjAcfV5IBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f=open(path+'/classes.json')\n",
        "data=json.load(f)"
      ],
      "metadata": {
        "id": "PaN3U5j55IDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "1aGSVnWh5IFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training=[]\n",
        "output=[]\n",
        "for intent in data['intents']:\n",
        "  count=0\n",
        "  for k in intent:\n",
        "    for v in intent.get(k):\n",
        "      for val in intent[k].get(v):\n",
        "        training.append(val)\n",
        "        output.append(count)\n",
        "    count+=1"
      ],
      "metadata": {
        "id": "ONR2nV3L5IIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training"
      ],
      "metadata": {
        "id": "ZgEMxrbZ5IKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "CNRSufkq5R8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.DataFrame({'sentences':training,'class':output})\n",
        "df.head(34)"
      ],
      "metadata": {
        "id": "KAZ5js8T5R_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(path+'/sentences.csv')"
      ],
      "metadata": {
        "id": "nlgtA9745SCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "sAVQsELa5SFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.sample(frac=1)\n",
        "df\n"
      ],
      "metadata": {
        "id": "Cd7aqa8u5INK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# df=pd.read_csv(path+'/sentence_shuff.csv')\n",
        "df.to_csv(path+'/shuffled_sent_2.csv')"
      ],
      "metadata": {
        "id": "tggto_635h8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_list=df['sentences'][:]\n",
        "data_list=list(data_list)"
      ],
      "metadata": {
        "id": "LlbRLaa_5h-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_list"
      ],
      "metadata": {
        "id": "tSyF9RhO5iAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "j4Xl2hZW5o9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "OIcKFWqp5o_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs=list(df['class'])\n",
        "outputs"
      ],
      "metadata": {
        "id": "yGXX9qIO5pB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lower_sent=[]\n",
        "for sen in data_list:\n",
        "  lower_sent.append(sen.lower())\n",
        "lower_sent"
      ],
      "metadata": {
        "id": "3K_r1kXq5pEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation"
      ],
      "metadata": {
        "id": "sOY2WSc35pGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newline_removed=[]\n",
        "for sen in lower_sent:\n",
        "  newline_removed.append(sen.replace('\\n',' '))\n",
        "newline_removed"
      ],
      "metadata": {
        "id": "eEC4a5np5y8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punc_removed=[]\n",
        "for sen in newline_removed:\n",
        "  for i in sen:\n",
        "    if i in punctuation:\n",
        "      sen=sen.replace(i,'')\n",
        "  punc_removed.append(sen)\n",
        "punc_removed"
      ],
      "metadata": {
        "id": "SrU01bAU5y-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "_TLouATq5zBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t='  hello dishant  hi be'\n",
        "print(t.split())"
      ],
      "metadata": {
        "id": "Ur1VawjX5zDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_removed=[]\n",
        "for sen in punc_removed:\n",
        "  current=[]\n",
        "  for word in sen.split():\n",
        "    if word not in stopwords.words('english'):\n",
        "      current.append(word)\n",
        "  stopwords_removed.append(current)\n",
        "stopwords_removed"
      ],
      "metadata": {
        "id": "kH5_PMX95zG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "stemmer=SnowballStemmer('english')\n",
        "vectorizer=TfidfVectorizer()"
      ],
      "metadata": {
        "id": "uhz5giwJ5-g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "lemm_sen=[]\n",
        "for sen in stopwords_removed:\n",
        "  current=[]\n",
        "  for word in sen:\n",
        "    current.append(lemmatizer.lemmatize(word))\n",
        "  lemm_sen.append(current)\n",
        "lemm_sen"
      ],
      "metadata": {
        "id": "9aVd8oNx5-jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_sent=[]\n",
        "for sen in lemm_sen:\n",
        "  current=[]\n",
        "  for word in sen:\n",
        "    current.append(stemmer.stem(word))\n",
        "  stemmed_sent.append(current)\n",
        "stemmed_sent"
      ],
      "metadata": {
        "id": "wd6925Ga6CjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete_sent=[]\n",
        "for sen in stemmed_sent:\n",
        "  current=''\n",
        "  for word in sen:\n",
        "    current=current+word+' '\n",
        "  complete_sent.append(current)\n",
        "complete_sent"
      ],
      "metadata": {
        "id": "QW-nUm4q6ClD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=vectorizer.fit_transform(complete_sent)\n",
        "print(vectorizer.get_feature_names())"
      ],
      "metadata": {
        "id": "gDCexGTb6CnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "N8YLZC4d6Cqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_indices=vectorizer.get_feature_names()\n",
        "feature_indices"
      ],
      "metadata": {
        "id": "pdDPrkEi5-lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_indices_dict={}\n",
        "for ind, word in enumerate(feature_indices):\n",
        "  feature_indices_dict[word]=ind\n",
        "feature_indices_dict"
      ],
      "metadata": {
        "id": "nHn6rAbe5-o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature_indices_dict={}\n",
        "\n",
        "import pickle\n",
        "with open(path+'/feature_indices_3.txt','wb') as f:\n",
        "  pickle.dump(feature_indices_dict,f)"
      ],
      "metadata": {
        "id": "_vX1Zqvg5pJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_values=X.toarray()\n",
        "tfidf_values"
      ],
      "metadata": {
        "id": "NvlCIFa75iC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_values[1][feature_indices_dict['mascot']]"
      ],
      "metadata": {
        "id": "lsRcXWWT5iFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path+'/tfidf_values_3.txt','wb') as f:\n",
        "  pickle.dump(tfidf_values,f)"
      ],
      "metadata": {
        "id": "GZ6daUJp6iVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tflearn"
      ],
      "metadata": {
        "id": "DD4RMQ376iXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tflearn"
      ],
      "metadata": {
        "id": "5AfKbSQZ6iZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape[1]"
      ],
      "metadata": {
        "id": "vvhKrGoG6ibA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.regularizers import l2"
      ],
      "metadata": {
        "id": "ZrfC7-ZO6idC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=models.Sequential()\n",
        "model.add(layers.Dense(100,input_dim=X.shape[1],activation='relu'))\n",
        "model.add(layers.Dense(100,activation='relu'))\n",
        "model.add(layers.Dense(50,activation='relu'))\n",
        "model.add(layers.Dense(7,activation='softmax'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Z5yPPNoI6qyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_values.shape"
      ],
      "metadata": {
        "id": "DWcaa7Im6q1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "output_np=np.array(outputs)\n",
        "output_np.shape"
      ],
      "metadata": {
        "id": "JGHg-BgU6q33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_tensor=tf.convert_to_tensor(tfidf_values)\n",
        "testing_tensor=tf.convert_to_tensor(output_np)"
      ],
      "metadata": {
        "id": "zQoIzwNJ6q7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_tensor"
      ],
      "metadata": {
        "id": "VB_R9MZ26igZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import SGD"
      ],
      "metadata": {
        "id": "5-SoPInb6zSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sgd=SGD()"
      ],
      "metadata": {
        "id": "9ENUol856zUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=sgd,loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics='accuracy')\n",
        "history=model.fit(training_tensor,testing_tensor,epochs=1000)"
      ],
      "metadata": {
        "id": "csqbPnCU6zWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics='accuracy')\n",
        "history=model.fit(training_tensor,testing_tensor,epochs=50)"
      ],
      "metadata": {
        "id": "UN-skVko6zZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(path+'/Induction_Chatbot_6.model')"
      ],
      "metadata": {
        "id": "2UuyuKad6zco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=input()\n",
        "predict(text)"
      ],
      "metadata": {
        "id": "Ya8X0YKc7CD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.weights"
      ],
      "metadata": {
        "id": "lwUvqNvn7CGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t=np.array([[1.5,2.5,3.5]])"
      ],
      "metadata": {
        "id": "pwEeyZlT7CJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l=[1,2,3,4,5]"
      ],
      "metadata": {
        "id": "P_hO1NVz7abG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l.insert(0,5)\n",
        "l"
      ],
      "metadata": {
        "id": "gyfYOAYh7adk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers=[[],[],[],[],[],[],[]]\n",
        "for intent in data['intents']:\n",
        "  for k, response in zip(intent.keys(),intent.values()):\n",
        "    print(k)\n",
        "    for r in response['responses']:\n",
        "      if k=='greetings':\n",
        "        answers[0].append(r)\n",
        "      elif k=='overview':\n",
        "        answers[1].append(r)\n",
        "      elif k=='conduct':\n",
        "        answers[2].append(r)\n",
        "      elif k=='academics':\n",
        "        answers[3].append(r)\n",
        "      elif k=='resources':\n",
        "        answers[4].append(r)\n",
        "      elif k=='activities':\n",
        "        answers[5].append(r)\n",
        "      elif k=='goodbye':\n",
        "        answers[6].append(r)"
      ],
      "metadata": {
        "id": "1uhLHdp57agC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers"
      ],
      "metadata": {
        "id": "OZBl0rvl7gWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path+'/answers_3.txt','wb') as f:\n",
        "  pickle.dump(answers,f)"
      ],
      "metadata": {
        "id": "v0OTJK7p7gYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers[0]"
      ],
      "metadata": {
        "id": "Cmq4pYhS7gbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=answers[6]\n",
        "lower_sent=[]\n",
        "for sent in sentences:\n",
        "  sent=sent.lower()\n",
        "  lower_sent.append(sent)\n",
        "punct_sent=[]\n",
        "for comm in lower_sent:\n",
        "  for w in comm:\n",
        "    if w in punctuation:\n",
        "      comm=comm.replace(w,'')\n",
        "  punct_sent.append(comm)\n",
        "comm_words=[]\n",
        "for comm in punct_sent:\n",
        "  comm_words.append(comm.split())\n",
        "cleaned_text=[]\n",
        "for comm in comm_words:\n",
        "  words=[]\n",
        "  for w in comm:\n",
        "    if w not in set(stopwords.words('english')):\n",
        "      words.append(w)\n",
        "  cleaned_text.append(words)\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "lemm_text=[]\n",
        "for text in cleaned_text:\n",
        "  words=[]\n",
        "  for w in text:\n",
        "    words.append(lemmatizer.lemmatize(w))\n",
        "  lemm_text.append(words)\n",
        "stemmer=SnowballStemmer('english')\n",
        "stem_text=[]\n",
        "for text in lemm_text:\n",
        "  words=''\n",
        "  for w in text:\n",
        "    words=words+stemmer.stem(w)+' '\n",
        "  stem_text.append(words)\n",
        "print(stem_text)"
      ],
      "metadata": {
        "id": "AmR1YXQm7geE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bot_responses=[]\n",
        "bot_responses.extend([stem_text])"
      ],
      "metadata": {
        "id": "yyk9kFAn7z7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bot_responses"
      ],
      "metadata": {
        "id": "sJg5k7967z9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path+'/bot_responses_3.txt','wb') as f:\n",
        "  pickle.dump(bot_responses,f)"
      ],
      "metadata": {
        "id": "k8y71GJ770Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bot_responses[0]"
      ],
      "metadata": {
        "id": "btSuK1KV70D6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers,models\n",
        "from string import punctuation\n",
        "import pickle\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "print(nltk.download('stopwords'))\n",
        "print(nltk.download('wordnet'))\n",
        "import numpy as np\n",
        "model=models.load_model(path+'/Induction_Chatbot_6.model')\n",
        "feature_indices_dict={}\n",
        "with open(path+'/feature_indices_3.txt','rb') as f:\n",
        "  feature_indices_dict=pickle.load(f)\n",
        "answers=[]\n",
        "with open(path+'/answers_3.txt','rb') as f:\n",
        "  answers=pickle.load(f)\n",
        "bot_responses=[]\n",
        "with open(path+'/bot_responses_3.txt','rb') as f:\n",
        "  bot_responses=pickle.load(f)"
      ],
      "metadata": {
        "id": "-27tCSNH7aic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "  sentences=[text]\n",
        "  # print(sentences)\n",
        "  lower_sent=[]\n",
        "  for sent in sentences:\n",
        "    sent=sent.lower()\n",
        "    lower_sent.append(sent)\n",
        "  punct_sent=[]\n",
        "  for comm in lower_sent:\n",
        "    for w in comm:\n",
        "      if w in punctuation:\n",
        "        comm=comm.replace(w,'')\n",
        "    punct_sent.append(comm)\n",
        "  comm_words=[]\n",
        "  for comm in punct_sent:\n",
        "    comm_words.append(comm.split())\n",
        "  cleaned_text=[]\n",
        "  for comm in comm_words:\n",
        "    words=[]\n",
        "    for w in comm:\n",
        "      if w not in set(stopwords.words('english')):\n",
        "        words.append(w)\n",
        "    cleaned_text.append(words)\n",
        "  lemmatizer=WordNetLemmatizer()\n",
        "  lemm_text=[]\n",
        "  for text in cleaned_text:\n",
        "    words=[]\n",
        "    for w in text:\n",
        "      words.append(lemmatizer.lemmatize(w))\n",
        "    lemm_text.append(words)\n",
        "  stemmer=SnowballStemmer('english')\n",
        "  stem_text=[]\n",
        "  for text in lemm_text:\n",
        "    words=''\n",
        "    for w in text:\n",
        "      words=words+stemmer.stem(w)+' '\n",
        "    words=words.replace('colleg','')\n",
        "    words=words.replace('facil','')\n",
        "    words=words.replace('campus','')\n",
        "    stem_text.append(words)\n",
        "  # print(stem_text[0])\n",
        "  classes={0:'greetings', 1:'overview',2:'conduct', 3:'academics', 4:'resources',5:'activities',6:'goodbye'}\n",
        "  vectorizer=TfidfVectorizer()\n",
        "  tfidf_matrix=vectorizer.fit_transform(stem_text)\n",
        "  X=tfidf_matrix.toarray()\n",
        "  feature_arr=vectorizer.get_feature_names()\n",
        "  feature_arr_dict={}\n",
        "  for ind,name in enumerate(feature_arr):\n",
        "    feature_arr_dict[name]=ind\n",
        "  test_tens=[[0]*784]\n",
        "  for i in feature_arr:\n",
        "    try:\n",
        "      test_tens[0][feature_indices_dict[i]]=X[0][feature_arr_dict[i]]\n",
        "    except:\n",
        "      pass\n",
        "  test_tens=tf.convert_to_tensor(np.array(test_tens))\n",
        "  predictions=model.predict(test_tens)\n",
        "  index=np.argmax(predictions)\n",
        "  responses_list=[]\n",
        "  responses_list=bot_responses[index][:]\n",
        "  responses_list.insert(0,stem_text[0])\n",
        "  count_vectorize=CountVectorizer().fit_transform(responses_list)\n",
        "  vectors=count_vectorize.toarray()\n",
        "  csim=cosine_similarity(vectors)\n",
        "  answer_index=np.argmax(csim[0][1:])\n",
        "  print(answers[index][answer_index])\n",
        "  return classes[index]"
      ],
      "metadata": {
        "id": "iK4C5ndz7al_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  try:\n",
        "    c=predict(input(\"You: \"))\n",
        "    print(\"\\n\")\n",
        "    if c=='goodbye':\n",
        "      break\n",
        "  except:\n",
        "    print(\"I'm good! How may I help you?\")\n",
        "    pass"
      ],
      "metadata": {
        "id": "k-iW2CrL8Rkq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}